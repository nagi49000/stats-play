{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1eb36136",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ba99af5",
   "metadata": {},
   "source": [
    "# Applying Kullback Leibler\n",
    "\n",
    "### Information entropy from probability distributions\n",
    "\n",
    "Recall from [1] and [2] the form of information entropy for a random variable $X$ with distribution $P(x)$ over a (measurable) space $x \\in \\mathcal{X}$\n",
    "\n",
    "$ I(P) = \\log_2 \\left( \\frac{1}{P(X)} \\right) $,\n",
    "\n",
    "$ H(P) = - \\sum_{x \\in \\mathcal{X}} P(x) \\log_2 P(x) $\n",
    "\n",
    "or more generally\n",
    "\n",
    "$ H(P) = \\mathbb{E}_{X\\sim P} [ - \\log_2 P(X) ] = \\mathbb{E}_{X\\sim P} [ I(X) ]$ \n",
    "\n",
    "$H(P)$ represents the 'uncertainty' of $X$, or the rate of generating information from an information source $X$. $I(P)$ represents the information content of an event from $X$. In the context of messaging, $I(P)$ represents the minimum length of the message (in bits) to transmit the information content of the message, and $H(P)$ is the expected length of a message.\n",
    "\n",
    "For a continuous random variable $X$ with probability density function $p(x)$ over a continuous space $x \\in \\mathcal{X}$, one can use differential entropy [4], which is NOT a $n\\to\\infty$ limit of information entropy \n",
    "\n",
    "$ H(p) = \\int_\\mathcal{X} dx p(x) \\log_2 p(x) $\n",
    "\n",
    "### Cross entropy from probability distributions\n",
    "\n",
    "The above definitions take into account only a single distribution over $X$. In practice, there may be multiple distributions over $X$, in particular an estimate (given a certain amount of evidence) of the distribution over $X$. Cross entropy allows extending the definitions of entropy to multiple distributions over the same random variable.\n",
    "\n",
    "Suppose now that messages/information content are generated by a sender obeying another distribution $Q(x)$. One then has\n",
    "\n",
    "$ I(Q) = \\log_2 \\left( \\frac{1}{Q(X)} \\right) $\n",
    "\n",
    "However, the true distribution is $P(X)$. This means that the expected message length to correctly convey the information is\n",
    "\n",
    "$ H(P,Q) := \\mathbb{E}_{X\\sim P} [ I(Q) ] = - \\sum_{x \\in \\mathcal{X}} P(x) \\log_2 Q(x) $\n",
    "\n",
    "The cross entropy is useful for considering the discrepancy between a true distribution, $P(X)$, and some candidate or estimated distribution $Q(X)$ that one might be working with. It represents the 'uncertainty' of $X$ when one is using a presumed distribution $Q(X)$. Some good exploratory examples are available here [5]. The extra uncertainty that arises over $H(P)$ can be used to define a quantity; namely the Kullback-Leibler divergence\n",
    "\n",
    "$ D_{KL}(P||Q) := H(P,Q) - H(P) $\n",
    "\n",
    "By Gibbs' inequality [6], one finds that $ D_{KL}(P||Q) \\geq 0 $ and $ D_{KL}(P||Q) = 0 $ if and only if $ P = Q $. $D_{KL}$ gives some kind of distance of how far $Q$ is from $P$, although does not form a distance metric, but does form a statistical divergence [7]."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "da976518",
   "metadata": {},
   "outputs": [],
   "source": [
    "def kl_div(p, q):\n",
    "    return  - np.sum(np.array(p) * np.log2(q)) + np.sum(np.array(p) * np.log2(p))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "bc3f32b5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KL divergence of a distribution from itself = 0.0\n",
      "KL divergence of a distribution from another = 3.5427017090192026\n"
     ]
    }
   ],
   "source": [
    "n_sample = 20\n",
    "p = np.random.random(n_sample)\n",
    "q = np.random.random(n_sample)\n",
    "print(f\"KL divergence of a distribution from itself = {kl_div(p, p)}\")\n",
    "print(f\"KL divergence of a distribution from another = {kl_div(p, q)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "966a6ac2",
   "metadata": {},
   "source": [
    "### References\n",
    "\n",
    "[1] Shannon, C. [\"A mathematical theory of communication\"](https://people.math.harvard.edu/~ctm/home/text/others/shannon/entropy/entropy.pdf)\n",
    "\n",
    "[2] Wikipedia, [\"Entropy\"](https://en.wikipedia.org/wiki/Entropy_(information_theory))\n",
    "\n",
    "[3] Wikipedia, [\"Conditional Entropy\"](https://en.wikipedia.org/wiki/Conditional_entropy)\n",
    "\n",
    "[4] Wikipedia, [\"Differential Entropy\"](https://en.wikipedia.org/wiki/Differential_entropy)\n",
    "\n",
    "[5] Towards Data Science, [\"Entropy, Cross-Entropy, and KL-Divergence Explained!\"](https://towardsdatascience.com/entropy-cross-entropy-and-kl-divergence-explained-b09cdae917a)\n",
    "\n",
    "[6] Wikipedia, [\"Gibbs' inequality\"](https://en.wikipedia.org/wiki/Gibbs%27_inequality#Proof)\n",
    "\n",
    "[7] Wikipedia, [\"Divergence_(statistics)\"](https://en.wikipedia.org/wiki/Divergence_(statistics))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
