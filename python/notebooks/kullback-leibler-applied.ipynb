{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e83a3873",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import copy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "285a5ffc",
   "metadata": {},
   "source": [
    "# Applying Kullback Leibler\n",
    "\n",
    "### Information entropy from probability distributions\n",
    "\n",
    "Recall from [1] and [2] the form of information entropy for a random variable $X$ with distribution $P(x)$ over a (measurable) space $x \\in \\mathcal{X}$\n",
    "\n",
    "$ I(P) = \\log_2 \\left( \\frac{1}{P(X)} \\right) $,\n",
    "\n",
    "$ H(P) = - \\sum_{x \\in \\mathcal{X}} P(x) \\log_2 P(x) $\n",
    "\n",
    "or more generally\n",
    "\n",
    "$ H(P) = \\mathbb{E}_{X\\sim P} [ - \\log_2 P(X) ] = \\mathbb{E}_{X\\sim P} [ I(X) ]$ \n",
    "\n",
    "$H(P)$ represents the 'uncertainty' of $X$, or the rate of generating information from an information source $X$. $I(P)$ represents the information content of an event from $X$. In the context of messaging, $I(P)$ represents the minimum length of the message (in bits) to transmit the information content of the message, and $H(P)$ is the expected length of a message.\n",
    "\n",
    "For a continuous random variable $X$ with probability density function $p(x)$ over a continuous space $x \\in \\mathcal{X}$, one can use differential entropy [4], which is NOT a $n\\to\\infty$ limit of information entropy \n",
    "\n",
    "$ H(p) = \\int_\\mathcal{X} dx p(x) \\log_2 p(x) $\n",
    "\n",
    "### Cross entropy from probability distributions\n",
    "\n",
    "The above definitions take into account only a single distribution over $X$. In practice, there may be multiple distributions over $X$, in particular an estimate (given a certain amount of evidence) of the distribution over $X$. Cross entropy allows extending the definitions of entropy to multiple distributions over the same random variable.\n",
    "\n",
    "Suppose now that messages/information content are generated by a sender obeying another distribution $Q(x)$. One then has\n",
    "\n",
    "$ I(Q) = \\log_2 \\left( \\frac{1}{Q(X)} \\right) $\n",
    "\n",
    "However, the true distribution is $P(X)$. This means that the expected message length to correctly convey the information is\n",
    "\n",
    "$ H(P,Q) := \\mathbb{E}_{X\\sim P} [ I(Q) ] = - \\sum_{x \\in \\mathcal{X}} P(x) \\log_2 Q(x) $\n",
    "\n",
    "The cross entropy is useful for considering the discrepancy between a true distribution, $P(X)$, and some candidate or estimated distribution $Q(X)$ that one might be working with. It represents the 'uncertainty' of $X$ when one is using a presumed distribution $Q(X)$. Some good exploratory examples of the cross-entropy are available here [5]. Using the cross-entropy for machine learning (where, by a mild abuse of definition, P is a delta function distribution obtained from the training label, and Q are the normalised output weights of the ML algo that are not necessarily a probability distribution) is described here [8]. The extra uncertainty that arises over $H(P)$ can be used to define a quantity; namely the Kullback-Leibler divergence\n",
    "\n",
    "$ D_{KL}(P||Q) := H(P,Q) - H(P) = \\sum_{x \\in \\mathcal{X}} P(x) \\log_2 \\left(\\frac{P(x)}{Q(x)}\\right) = \\sum_{x \\in \\mathcal{X}} P(x) \\log_2 P(x) - P(x) \\log_2 Q(x) $\n",
    "\n",
    "By Gibbs' inequality [6], one finds that $ D_{KL}(P||Q) \\geq 0 $ and $ D_{KL}(P||Q) = 0 $ if and only if $ P = Q $. $D_{KL}$ gives some kind of distance of how far $Q$ is from $P$, although does not form a distance metric, but does form a statistical divergence [7]."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "12f4ab18",
   "metadata": {},
   "outputs": [],
   "source": [
    "def simple_kl_div(p: np.array, q: np.array) -> float:\n",
    "    return  np.sum(np.array(p) * np.log2(np.array(p)/np.array(q)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "70afe653",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KL divergence of a distribution from itself = 0.0\n",
      "KL divergence of a distribution from another = 0.9975613123529624\n"
     ]
    }
   ],
   "source": [
    "n_class = 20\n",
    "p = np.random.random(n_class)\n",
    "p /= np.sum(p)\n",
    "q = np.random.random(n_class)\n",
    "q /= np.sum(q)\n",
    "print(f\"KL divergence of a distribution from itself = {simple_kl_div(p, p)}\")\n",
    "print(f\"KL divergence of a distribution from another = {simple_kl_div(p, q)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fee9bc20",
   "metadata": {},
   "source": [
    "### Using KL divergence in practice\n",
    "\n",
    "Whilst the continuous and discrete forms can be used on theoretical distributions, often the discrete form is applied (on probability mass functions) to an empirical distribution, and/or a binned distribution. There are some issues in direct calculation, since the formula contains a logarithm that can blow up at zero. In general, the discrete form is fairly easy to apply (see simple_kl_div above), unless there are any $x_i$ such that $Q(x_i) = 0$ and $P(x_i) \\neq 0$. The cases of $P(x_i) = 0$ are easy to address, since the logarithm containing $P(x)$ is multiplied by $P(x)$, and\n",
    "\n",
    "$ \\lim_{x \\rightarrow 0+} x \\log x = 0  $"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "538f9dd6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KL divergence of a distribution from another = 0.8698209175520053\n"
     ]
    }
   ],
   "source": [
    "def p_regularized_kl_div(p: np.array, q: np.array, p_eps: float=1.0e-16) -> float:\n",
    "    \"\"\" p - array of length 'd' representing a true discrete probability distribution over d outcomes\n",
    "        q - array of length 'd' representing a postulated discrete probability distribution over d outcomes\n",
    "        p_eps - in calculating H(P), a regularization term to allow p_i log p_i to evaluate to zero whenever\n",
    "                p_i < p_eps; here just to stop log p_i blowing up\n",
    "                \n",
    "        returns D_KL(P||Q) = H(P,Q) - H(P)\n",
    "    \"\"\"\n",
    "    # for very small p, remove values that will not contribute to the H(P) sum\n",
    "    # since lim (x -> 0+) x log x = 0, and a direct eval of log 0 will blow up\n",
    "    p_reduced = [x for x in p if x > p_eps]\n",
    "    h_p = - np.sum(np.array(p_reduced) * np.log2(np.array(p_reduced)))\n",
    "    # perform naive calculation for H(P,Q)\n",
    "    h_pq = - np.sum(np.array(p) * np.log2(np.array(q)))\n",
    "    return h_pq - h_p\n",
    "\n",
    "p[0] = 0.0  # put in a pathological 0 for p\n",
    "p /= np.sum(p)\n",
    "print(f\"KL divergence of a distribution from another = {p_regularized_kl_div(p, q)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b9be6dd",
   "metadata": {},
   "source": [
    "The blow up from $\\{x_i | Q(x_i) = 0, P(x_i) \\neq 0\\}$ is, from a theoretical perspective, perhaps unsurprising. $Q(x_i) = 0$ means that the assumed distribution asserts that $x_i$ can never happen. However, $P(x_i) \\neq 0$ means that in truth $x_i$ can happen, revealing an 'infinite' amount of 'uncertainty' or 'surprise' around $x_i$. This means that something will have to be done to the assumed distribution $Q$ in order to get well-defined answers."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15a98b8b",
   "metadata": {},
   "source": [
    "### Applying smoothing to the Q distribution\n",
    "\n",
    "'Smoothing' on an array is a generic term for removing higher frequency components. In general, it may or may not be a good idea, depending on the domain.\n",
    "\n",
    "Laplace/Lidstone/additive smoothing [9] is a method that can be applied to a distribution over a categorical dataset, which comes from Laplace's sunrise problem, of our observation samples always have the sun rising the next day; what happens if the sun does not rise on some day in the future?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9c733ab0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def kl_div(\n",
    "    p: np.array, \n",
    "    q: np.array, \n",
    "    p_eps: float=1.0e-16, \n",
    "    q_smoothing: str=None, \n",
    "    n_sample: int=None, \n",
    "    additive_smoothing_alpha: float=1.0\n",
    ") -> float:\n",
    "    \"\"\" p - array of length 'd' representing a true discrete probability distribution over d outcomes\n",
    "        q - array of length 'd' representing a postulated discrete probability distribution over d outcomes\n",
    "        p_eps - in calculating H(P), a regularization term to allow p_i log p_i to evaluate to zero whenever\n",
    "                p_i < p_eps; here just to stop log p_i blowing up\n",
    "        q_smoothing: type of smoothing to apply to q distribution\n",
    "        n_sample: the number of samples involved in calculating the empirical distribution q\n",
    "        additive_smoothing_alpha: alpha to use in smoothing https://en.wikipedia.org/wiki/Additive_smoothing\n",
    "                                  actual value to use is contentious, although 1 tends to be the accepted value\n",
    "    \"\"\"\n",
    "    d = len(p)\n",
    "    if len(q) != d:\n",
    "        raise ValueError(f\"p and q must be of the same length, len(p) = {len(p)}, len(q) = {len(q)}\")\n",
    "    # for very small p, remove values that will not contribute to the H(P) sum\n",
    "    # since lim (x -> 0+) x log x = 0, and a direct eval of log 0 will blow up\n",
    "    p_reduced = [x for x in p if x > p_eps]\n",
    "    h_p = - np.sum(np.array(p_reduced) * np.log2(np.array(p_reduced)))\n",
    "    # perform naive calculation for H(P,Q)\n",
    "    if q_smoothing is None:\n",
    "        pass\n",
    "    elif q_smoothing == \"additive\":\n",
    "        # assume that q has already been normalised from an empirical distribution, so undo that\n",
    "        q_smoothed = n_sample * q\n",
    "        # apply additive smoothing formula\n",
    "        q_smoothed = (q_smoothed + additive_smoothing_alpha) / (n_sample + additive_smoothing_alpha * d)\n",
    "        q = q_smoothed\n",
    "    else:\n",
    "        raise ValueError(f\"unknown q-smoothing requested: {q_smoothing}\")\n",
    "    h_pq = - np.sum(np.array(p) * np.log2(np.array(q)))\n",
    "    return h_pq - h_p"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "88a1eefa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KL divergence of a distribution from another = 0.8698209175520044\n"
     ]
    }
   ],
   "source": [
    "p_pathological = copy.deepcopy(p)\n",
    "p_pathological[0] = 0.0  # put in a pathological 0 for p\n",
    "p_pathological /= np.sum(p_pathological)\n",
    "print(f\"KL divergence of a distribution from another = {kl_div(p_pathological, q)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d2871e04",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KL divergence of a distribution from another = 0.8078646012208632\n"
     ]
    }
   ],
   "source": [
    "q_pathological = copy.deepcopy(q)\n",
    "q_pathological[0] = 0.0 # put in a pathological 0 for q\n",
    "q_pathological /= np.sum(q_pathological)\n",
    "print(f\"KL divergence of a distribution from another = {kl_div(p_pathological, q_pathological, q_smoothing='additive', n_sample=1000)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02d776ae",
   "metadata": {},
   "source": [
    "### References\n",
    "\n",
    "[1] Shannon, C. [\"A mathematical theory of communication\"](https://people.math.harvard.edu/~ctm/home/text/others/shannon/entropy/entropy.pdf)\n",
    "\n",
    "[2] Wikipedia, [\"Entropy\"](https://en.wikipedia.org/wiki/Entropy_(information_theory))\n",
    "\n",
    "[3] Wikipedia, [\"Conditional Entropy\"](https://en.wikipedia.org/wiki/Conditional_entropy)\n",
    "\n",
    "[4] Wikipedia, [\"Differential Entropy\"](https://en.wikipedia.org/wiki/Differential_entropy)\n",
    "\n",
    "[5] Towards Data Science, [\"Entropy, Cross-Entropy, and KL-Divergence Explained!\"](https://towardsdatascience.com/entropy-cross-entropy-and-kl-divergence-explained-b09cdae917a)\n",
    "\n",
    "[6] Wikipedia, [\"Gibbs' inequality\"](https://en.wikipedia.org/wiki/Gibbs%27_inequality#Proof)\n",
    "\n",
    "[7] Wikipedia, [\"Divergence_(statistics)\"](https://en.wikipedia.org/wiki/Divergence_(statistics))\n",
    "\n",
    "[8] Medium, [\"Loss Functions in Machine Learning\"](https://medium.com/swlh/cross-entropy-loss-in-pytorch-c010faf97bab)\n",
    "\n",
    "[9] Wikipedia, [\"Additive smoothing\"](https://en.wikipedia.org/wiki/Additive_smoothing)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
