{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fb96061a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy.stats\n",
    "import random\n",
    "import math\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85d3d3bd",
   "metadata": {},
   "source": [
    "# T-tests\n",
    "\n",
    "This notebook will ecplore some simple cases of hypothesis testing, in particular t-tests, and how to perform them in Python.\n",
    "\n",
    "## Z-tests\n",
    "\n",
    "Before moving onto t-tests, first conisder the simpler, and probably more familiar case of Z-tests.\n",
    "\n",
    "### One-sample Z-test\n",
    "\n",
    "This is perhaps the most familiar entry level hypothesis test, and the test is based on whether or not a sample comes from a known Normal distribution. Consider that we have a candidate distribution\n",
    "\n",
    "$ X \\sim N(\\mu, \\sigma^2) $\n",
    "\n",
    "For the sake of being explicit, say $\\mu = 10, \\sigma=3$. If the sample we had was $x = 7.1$, then the \"z-score\" can be calculated as $(x - \\mu ) / \\sigma $"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d6b4a686",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "z-score = -0.9666666666666668\n"
     ]
    }
   ],
   "source": [
    "z_score = (7.1 - 10)/3\n",
    "print(f\"z-score = {z_score}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec68ee20",
   "metadata": {},
   "source": [
    "One might perform a one or two tailed test based on the null and alternative hypotheses, e.g. if our test stat comes from a distribution $X' \\sim N(\\mu', \\sigma) $\n",
    "\n",
    "$H_0: \\mu' = \\mu, H_1: \\mu' \\neq \\mu$\n",
    "\n",
    "would be a two-tailed test, and \n",
    "\n",
    "$H_0: \\mu' = \\mu, H_1: \\mu' < \\mu$\n",
    "\n",
    "would be a one-tailed test. \n",
    "\n",
    "Since the candidate distribution is $N(\\mu, \\sigma^2)$, the null hypothesis implicitly assumes the z-score is distributed $N(0,1)$.\n",
    "\n",
    "From the z-score, one can calculate the p-value, which is the probability of getting a value more extreme than the test value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8423e18e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "two-tailed p-value = 0.33371069574356604\n"
     ]
    }
   ],
   "source": [
    "p_value_2tail = scipy.stats.norm.sf(abs(z_score)) * 2\n",
    "print(f\"two-tailed p-value = {p_value_2tail}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4cc33572",
   "metadata": {},
   "source": [
    "So there is a 0.33 probability of a more extreme sample occuring. At a 10% significance level, if the probability of a more extreme sample were less than 0.1, then the test sample would be so extreme that the null hypothesis would be rejected (in this case, the null hypthesis is not rejected).\n",
    "\n",
    "For a one-tailed test, the p-value would be calculated as"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ef64df53",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "one-tailed p-value = 0.16685534787178302\n"
     ]
    }
   ],
   "source": [
    "p_value_1tail = scipy.stats.norm.sf(abs(z_score))\n",
    "print(f\"one-tailed p-value = {p_value_1tail}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9900d8f0",
   "metadata": {},
   "source": [
    "### Multi-sample Z-test"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e17cde8",
   "metadata": {},
   "source": [
    "If instead of one sample, there are multiple samples, and one is concerned with whether or not the mean of those samples comes from the candidate distribution. This is essentially a one sample test, but makes use of the fact that if\n",
    "\n",
    "$ X, Y \\sim N(\\mu, \\sigma^2) $\n",
    "\n",
    "and are independent then\n",
    "\n",
    "$ X + Y \\sim N(2\\mu, 2\\sigma^2) $\n",
    "\n",
    "and for a constant, $k$,\n",
    "\n",
    "$ kX \\sim N(k\\mu, k^2\\sigma^2) $\n",
    "\n",
    "In the case of having multiple test samples, and assuming they are independent (and from the same candidate distribution), one can calculate the mean of the test samples, $\\bar{x}$ which under the null hypothesis will have a distribution\n",
    "\n",
    "$ \\frac{1}{n} \\sum_{i=1}^n X_i \\sim N(\\mu, \\sigma^2 / n)$\n",
    "\n",
    "In this case the z-score can be calculated as\n",
    "\n",
    "$ \\textrm{z score} = \\frac{\\bar{x} - \\mu}{\\sigma / \\sqrt{n}}$\n",
    "\n",
    "and the same principles for one and two tailed tests can be applied. This tests assumes $\\sigma$ and $\\mu$ are known. $\\mu$ is often a constants taken to be related to the test hypothesis/business problem."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78eed3b3",
   "metadata": {},
   "source": [
    "### Difference between two sample sets Z-test\n",
    "This is similar to the multi-sample Z-test, except now there are two sets of multi-sample data to be compared (e.g. test stats collected from A/B testing).\n",
    "\n",
    "As before, one can calculate the sample mean of the two sample sets. This test is usually concerned with the difference between the two sample sets, e.g. a null hypothesis of the candidate distributions for each sample-set having the same mean.\n",
    "\n",
    "For a pair of samples sets $\\mathcal{X}_1, \\mathcal{X}_2$, which contain i.i.d. samples coming from distributions\n",
    "\n",
    "$ X_i \\sim N(\\mu_i, \\sigma_i^2) $,\n",
    "\n",
    "One can compute a z-score as\n",
    "\n",
    "$ \\textrm{z score} = \\frac{(\\bar{x}_1 - \\bar{x}_2) - (\\mu_1 - \\mu_2)}{\\sqrt{\\frac{\\sigma_1^2}{n_1} + \\frac{\\sigma_2^2}{n_2}}}$\n",
    "\n",
    "Due to the identity\n",
    "\n",
    "$ aX_1 + bX_2 \\sim N(a\\mu_1 + b\\mu_2, a^2\\sigma_1^2 + b^2\\sigma_2^2) $\n",
    "\n",
    "under the null hypothesis, the z-score will be distributed $N(0,1)$. One can then go ahead and perform hypothesis tests. The same principles for one and two tailed tests can be applied. This tests assumes $\\sigma_i$ and $\\mu_i$ are known. The $\\mu_i$ are constants often related to the test hypothesis/business problem. In the case of A/B testing, the null hypothesis is often $\\mu_1 - \\mu_2 = 0$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a3af4a7",
   "metadata": {},
   "source": [
    "## T-tests\n",
    "\n",
    "### Central limit theorems\n",
    "\n",
    "Z-tests assume that the candidate distribution is Normal. When is this justified? In multi-sample scenarios, one often calls on a \"Central Limit Theorem\", to say that a sample mean (no matter the original distribution) of $n$ samples will asymptotically approach a Normal distribution as $n \\to \\infty$. There is a subtelty on the actual conditions, how quickly the sample mean distribution actually approaches a Normal distirbution (as a function of $n$), and any other conditions. In practice, a central limit theorem applies to each possible distribution, some examples of which are [here](https://sphweb.bumc.bu.edu/otlt/mph-modules/bs/bs704_probability/BS704_Probability12.html). In practice, the original underlying distributions are unknown, so statisticians usually go along with \"The Central Limit Theorem holds when $n \\geq 30$\".\n",
    "\n",
    "The assumptions of the above Z-tests can fail if\n",
    "- a sample mean distribution is ill-defined\n",
    "- the sample points are not independent\n",
    "- the number of samples is not sufficiently large\n",
    "- the variances of the underlying distributions are unknown \n",
    "\n",
    "([this](https://bytepawn.com/beyond-the-central-limit-theorem.html) covers the failure cases in more detail). One can address the third and fourth cases by performing a t-test instead of a z-test. \n",
    "\n",
    "[this](https://www.analyticsvidhya.com/blog/2020/06/statistics-analytics-hypothesis-testing-z-test-t-test/) covers some choices of the flavour of test one might choose from a practical perspective.\n",
    "\n",
    "### Multi-sample T-tests\n",
    "\n",
    "In this case, instead of a z-score, one calculates a t-score. The null hypothesis assumes that the t-score is sampled from a [t-distribution](https://en.wikipedia.org/wiki/Student%27s_t-distribution).\n",
    "\n",
    "A t-score is calculated similarly to a z-score, but using the sample standard deviation\n",
    "\n",
    "$ \\textrm{t score} = \\frac{\\bar{x} - \\mu}{s / \\sqrt{n}}$\n",
    "\n",
    "where\n",
    "\n",
    "$ s^2 = \\frac{1}{n-1} \\sum_{i=1}^n (x_i - \\bar{x})^2 $\n",
    "\n",
    "In the example below,\n",
    "\n",
    "$H_0: \\mu = 0.1$\n",
    "\n",
    "$H_1: \\mu > 0.1$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9ce80641",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "t-score = 3.6387384717963607\n"
     ]
    }
   ],
   "source": [
    "random.seed(42)\n",
    "n = 15\n",
    "\n",
    "samples = [random.random() for _ in range(n)]  # samples from U[0,1)\n",
    "xbar = sum(samples) / n\n",
    "s = math.sqrt(sum([(x-xbar)**2 for x in samples])/(n-1))\n",
    "t_score = (xbar - 0.1) / (s / math.sqrt(n))\n",
    "print(f\"t-score = {t_score}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "432f14ac",
   "metadata": {},
   "source": [
    "This can be used to calculate a p-value for a one-tailed test (since the t-distribution is symmetric, a two-tailed test would increase the p-value by a factor of 2). The degrees of freedom (since the test assumes that the input samples are independent) is $n-1$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "522b7728",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "one-tailed p-value = 0.0013420587507718997\n"
     ]
    }
   ],
   "source": [
    "p_value_1tail = scipy.stats.t.sf(abs(t_score), df=n-1)\n",
    "print(f\"one-tailed p-value = {p_value_1tail}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "275c898d",
   "metadata": {},
   "source": [
    "The probability of a more extreme example is 0.0013; at the 10% significance level, this is less likely than 0.1, and so the null hypothesis is rejected. \n",
    "\n",
    "scipy.stats can directly compute the statistic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "34d64e31",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Ttest_1sampResult(statistic=3.6387384717963607, pvalue=0.0013420587507718997)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scipy.stats.ttest_1samp(samples, 0.1, alternative=\"greater\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86f701bd",
   "metadata": {},
   "source": [
    "### Difference between two sample sets T-test - unequal variances\n",
    "\n",
    "This follows the same principle as the Z-test example, except now the test statistic uses the sample standard deviation, as opposed to a pre-defined standard deviation\n",
    "\n",
    "$ \\textrm{t score} = \\frac{(\\bar{x}_1 - \\bar{x}_2) - (\\mu_1 - \\mu_2)}{\\sqrt{\\frac{s_1^2}{n_1} + \\frac{s_2^2}{n_2}}}$\n",
    "\n",
    "There is a breakdown of nomenclature here; many times one comes across a two sample-set T-test, the test assumes that the variances are the same (which needn't be the case). The version here may be called in the literature _Welch's T-test_, or _unequal variances T-test_. The variations one might encounter are explained [here](https://en.wikipedia.org/wiki/Student%27s_t-test).\n",
    "\n",
    "In this case, there will be a pair of sample sets to compare/test against hypotheses, e.g. for A/B testing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "502b8a46",
   "metadata": {},
   "outputs": [],
   "source": [
    "random.seed(43)\n",
    "n_1 = 11\n",
    "n_2 = 16\n",
    "samples_1 = [random.random() for _ in range(n_1)]  # samples from U[0,1)\n",
    "# samples from U[-0.5,1.5), so that the var is different from samples_1\n",
    "samples_2 = [random.random()*2 - 0.5 for _ in range(n_2)]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "977fd9b4",
   "metadata": {},
   "source": [
    "The null and alternative hypotheses will typically be represented by assumptions on $\\mu_i$, e.g.\n",
    "\n",
    "$H_0: \\mu_1 = \\mu_2$\n",
    "\n",
    "$H_1: \\mu_1 \\neq \\mu_2$\n",
    "\n",
    "As before, one can go ahead and compute the test statistic based on the null hypothesis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "62738b19",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "t-score = -1.2797528973559824\n"
     ]
    }
   ],
   "source": [
    "xbar_1 = sum(samples_1)/n_1\n",
    "xbar_2 = sum(samples_2)/n_2\n",
    "\n",
    "s_1_sqr = sum([(x-xbar_1)**2 for x in samples_1])/(n_1-1)\n",
    "s_2_sqr = sum([(x-xbar_2)**2 for x in samples_2])/(n_2-1)\n",
    "\n",
    "t_score = (xbar_1 - xbar_2) / math.sqrt( s_1_sqr/n_1 + s_2_sqr/n_2 )\n",
    "print(f\"t-score = {t_score}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ce969f3",
   "metadata": {},
   "source": [
    "In the case of Welch's t-test, there is a subtelty that the test statistic is not quite t-distributed. In simpler forms of the two-sample-set T-test, the test statistic (under the null hypothesis) is t-distributed, and the degrees of freedom is $(n_1 - 1) + (n_2 - 1)$. In the case of Welch's t-test, the test statistic is approximated by a t-distribution with degrees of freedom\n",
    "\n",
    "$ \\textrm{d.o.f} =  \\frac{\\left(\\frac{s_1^2}{n_1} + \\frac{s_2^2}{n_2}\\right)^2}{\\frac{(s_1^2/n_1)^2}{n_1-1}+\\frac{(s_2^2/n_2)^2}{n_2-1}}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "647f376d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_welchs_ttest_dof(s1sqr, s2sqr, n1, n2):\n",
    "    numerator = (s1sqr/n1 + s2sqr/n2) ** 2\n",
    "    denominator = (s1sqr/n1) ** 2 / (n1 - 1) + (s2sqr/n2) ** 2 / (n2 - 1)\n",
    "    return numerator / denominator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b7be6892",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "degrees of freedom = 24.369240640703026\n"
     ]
    }
   ],
   "source": [
    "dof = get_welchs_ttest_dof(s_1_sqr, s_2_sqr, n_1, n_2)\n",
    "print(f\"degrees of freedom = {dof}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78694587",
   "metadata": {},
   "source": [
    "Note that this value is a little different to what $n_1 + n_2 -2$ would have given. \n",
    "\n",
    "Based on the hypotheses, this is a 2-tailed test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "14f75359",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "two-tailed p-value = 0.2126820106340286\n"
     ]
    }
   ],
   "source": [
    "p_value_2tail = scipy.stats.t.sf(abs(t_score), df=dof) * 2\n",
    "print(f\"two-tailed p-value = {p_value_2tail}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "662e6c19",
   "metadata": {},
   "source": [
    "scipy.stats can directly compute the statistic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "916eb9cd",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Ttest_indResult(statistic=-1.2797528973559833, pvalue=0.2126820106340284)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# set equal_var to False, since the two sample sets do not have equal variance; this does a Welch's test\n",
    "scipy.stats.ttest_ind(samples_1, samples_2, equal_var=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45f92f51",
   "metadata": {},
   "source": [
    "Going along with the null hypothesis, the probability of getting a more extreme result is 0.213; using a significance level of 10%, the resultant p-value is not extreme enough for the null hypothesis to be rejected."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b5231fc",
   "metadata": {},
   "source": [
    "### Difference between two sample sets T-test - equal variances\n",
    "\n",
    "In the case of the two sample sets sharing the same variance, a test statistic that actually follows a t-distribution can be calculated, as [here](https://en.wikipedia.org/wiki/Student%27s_t-test#Equal_or_unequal_sample_sizes,_similar_variances_(1/2_%3C_sX1/sX2_%3C_2))."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "30a71cf0",
   "metadata": {},
   "outputs": [],
   "source": [
    "random.seed(43)\n",
    "n_1 = 11\n",
    "n_2 = 16\n",
    "samples_1 = [random.random() for _ in range(n_1)]  # samples from U[0,1)\n",
    "samples_2 = [random.random() for _ in range(n_2)]  # samples from U[0,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "f20a2111",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "t-score = -1.3797269616522334\n"
     ]
    }
   ],
   "source": [
    "xbar_1 = sum(samples_1)/n_1\n",
    "xbar_2 = sum(samples_2)/n_2\n",
    "\n",
    "s_1_sqr = sum([(x-xbar_1)**2 for x in samples_1])/(n_1-1)\n",
    "s_2_sqr = sum([(x-xbar_2)**2 for x in samples_2])/(n_2-1)\n",
    "\n",
    "dof = n_1 + n_2 - 2\n",
    "s_pooled = math.sqrt( ((n_1-1)*s_1_sqr + (n_2-1)*s_2_sqr)/dof)\n",
    "t_score = (xbar_1 - xbar_2) / (s_pooled * math.sqrt(1.0/n_1 + 1.0/n_2))\n",
    "print(f\"t-score = {t_score}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "3feaa1d1",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "two-tailed p-value = 0.1798872270104553\n"
     ]
    }
   ],
   "source": [
    "p_value_2tail = scipy.stats.t.sf(abs(t_score), df=dof) * 2\n",
    "print(f\"two-tailed p-value = {p_value_2tail}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "840f44f9",
   "metadata": {},
   "source": [
    "which can be obtained from scipy.stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "81623652",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Ttest_indResult(statistic=-1.3797269616522334, pvalue=0.1798872270104553)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scipy.stats.ttest_ind(samples_1, samples_2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "175f4db7",
   "metadata": {},
   "source": [
    "Note that this gives a slightly different answer to Welch's test, since that test is an approximation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "c68335ef",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Ttest_indResult(statistic=-1.3453442754218516, pvalue=0.1938014455473255)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# DANGER WILL ROBINSON - when the variances are the same, there is no need to use this approximation\n",
    "scipy.stats.ttest_ind(samples_1, samples_2, equal_var=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5114a9c",
   "metadata": {},
   "source": [
    "### Difference between two sample sets T-test - dependent tests\n",
    "\n",
    "This is a special case of two sample sets, where the relation between the sets is not so much A-set versus B-set, but that the sets are the same size, and represent the same population entities under different conditions (e.g. a class of students who do an exam and get exam scores, and then the same set of students doing an exam a year later).\n",
    "\n",
    "Consider such a set of results, and the expectation that the increase in marks should be 5 points."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "4f96d5ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "random.seed(44)\n",
    "n = 15\n",
    "\n",
    "results_1 = [int(random.random() * 100) for _ in range(n)]  # uniform random ints in [0,100)\n",
    "results_2 = [int(random.random() * 100) for _ in range(n)]  # uniform random ints in [0,100)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5dcf44ac",
   "metadata": {},
   "source": [
    "In this artificially created dataset, the student at index \"i\" in results_1 is the same student at index \"i\" in results_2; the relevant entry in results_2 is meant to be the same student's result when taking the exam a year later.\n",
    "\n",
    "Based on the expectation of a result increase, one can construct a hypothesis test\n",
    "\n",
    "$H_0:$ the difference between results_2 and results_1 has mean 5\n",
    "\n",
    "$H_1:$ the difference between results_2 and results_1 has mean < 5\n",
    "\n",
    "The t-score can be calculated as done [here](https://en.wikipedia.org/wiki/Student%27s_t-test#Dependent_t-test_for_paired_samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "01d9c745",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "t-score = -0.23560440840463792\n"
     ]
    }
   ],
   "source": [
    "deltas = np.array(results_2) - np.array(results_1)\n",
    "xbar = sum(deltas)/n\n",
    "s = np.std(deltas, ddof=1)  # sample std deviation, courtesy of numpy\n",
    "mu0 = 5  # the 5 from the null hypothesis\n",
    "\n",
    "t_score = (xbar - mu0)/(s/math.sqrt(n))\n",
    "print(f\"t-score = {t_score}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "4b871e38",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "one-tailed p-value = 0.4085756326449204\n"
     ]
    }
   ],
   "source": [
    "p_value_1tail = scipy.stats.t.sf(abs(t_score), df=n-1)\n",
    "print(f\"one-tailed p-value = {p_value_1tail}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a943f39f",
   "metadata": {},
   "source": [
    "which can also be computed directly using scipy.stats (which doesn't have an entry for placing $\\mu_0$, so the value for $\\mu_0$ has been subtracted from the second set of results, and the scipy.stats.ttest_rel function \"sees\" a problem of $H_0: \\mu=0, H_1: \\mu < 0$)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "9620c8fc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Ttest_relResult(statistic=-0.23560440840463792, pvalue=0.4085756326449204)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scipy.stats.ttest_rel(np.array(results_2)-mu0, results_1, alternative=\"less\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6249c86",
   "metadata": {},
   "source": [
    "At a 10% significance level, the p-value is not sufficiently extreme for the null hypothesis to be rejected."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98872ac5",
   "metadata": {},
   "source": [
    "## References\n",
    "\n",
    "[1] _Student's t-test_ https://en.wikipedia.org/wiki/Student%27s_t-test\n",
    "\n",
    "[2] _Welchâ€“Satterthwaite equation_ https://en.wikipedia.org/wiki/Welch%E2%80%93Satterthwaite_equation\n",
    "\n",
    "[3] _Beyond the Central Limit Theorem_ https://bytepawn.com/beyond-the-central-limit-theorem.html\n",
    "\n",
    "[4] _Central Limit Theorem_\n",
    "https://sphweb.bumc.bu.edu/otlt/mph-modules/bs/bs704_probability/BS704_Probability12.html\n",
    "\n",
    "[5] _Hypothesis Testing and Z-Test vs. T-Test_ https://www.analyticsvidhya.com/blog/2020/06/statistics-analytics-hypothesis-testing-z-test-t-test/"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
